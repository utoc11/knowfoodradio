#!/usr/bin/env python3
"""
ç°¡æ˜“ç‰ˆæ´¾ç”Ÿãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ

æ–‡å­—èµ·ã“ã—ã‹ã‚‰åŸºæœ¬çš„ãªæ´¾ç”Ÿãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆã—ã¾ã™ã€‚
Claude APIãŒåˆ©ç”¨ã§ããªã„å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ã€‚
"""

import json
import os
import re
from pathlib import Path
from datetime import datetime
import sys


BASE_DIR = Path(__file__).parent
FROM_RSS_DIR = BASE_DIR / "from-rss"


def parse_srt(srt_content: str) -> list:
    """SRTãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ‘ãƒ¼ã‚¹ã—ã¦ã€ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã¨ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º"""
    entries = []
    lines = srt_content.strip().split('\n')
    i = 0
    
    while i < len(lines):
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ã‚¹ã‚­ãƒƒãƒ—
        if lines[i].strip().isdigit():
            i += 1
            
            # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—
            if i < len(lines) and ' --> ' in lines[i]:
                time_match = re.match(r'(\d{2}:\d{2}:\d{2}),\d{3} --> (\d{2}:\d{2}:\d{2}),\d{3}', lines[i])
                if time_match:
                    start_time = time_match.group(1)
                    i += 1
                    
                    # ãƒ†ã‚­ã‚¹ãƒˆï¼ˆè¤‡æ•°è¡Œã®å ´åˆã‚ã‚Šï¼‰
                    text_lines = []
                    while i < len(lines) and lines[i].strip() and not lines[i].strip().isdigit():
                        text_lines.append(lines[i].strip())
                        i += 1
                    
                    if text_lines:
                        entries.append({
                            'time': start_time,
                            'text': ' '.join(text_lines)
                        })
        else:
            i += 1
    
    return entries


def extract_speakers(text: str) -> list:
    """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰è©±è€…ã‚’æŠ½å‡º"""
    speakers = []
    if 'TTã§ã™' in text or 'TT:' in text:
        speakers.append('TTã•ã‚“')
    if 'ã‚†ã¨ã§ã™' in text or 'ã‚†ã¨:' in text:
        speakers.append('ã‚†ã¨ã•ã‚“')
    # ã‚²ã‚¹ãƒˆã®æ¤œå‡ºï¼ˆã€Œã•ã‚“ã€ã§çµ‚ã‚ã‚‹å ´åˆãŒå¤šã„ï¼‰
    guest_pattern = r'([ã-ã‚“ã‚¡-ãƒ¶ãƒ¼ä¸€-é¾ ]+)ã•ã‚“'
    matches = re.findall(guest_pattern, text)
    for match in matches:
        if match not in ['TT', 'ã‚†ã¨'] and len(match) > 1:
            speakers.append(f'{match}ã•ã‚“')
    return list(set(speakers))


def generate_summary(entries: list, metadata: dict) -> str:
    """è¦ç´„ã‚’ç”Ÿæˆ"""
    title = metadata.get('title', '')
    
    # æœ€åˆã®5åˆ†é–“ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’çµåˆ
    early_text = ' '.join([e['text'] for e in entries[:50]])
    
    # ã‚²ã‚¹ãƒˆã®æ¤œå‡º
    speakers = extract_speakers(early_text)
    guest = None
    for speaker in speakers:
        if speaker not in ['TTã•ã‚“', 'ã‚†ã¨ã•ã‚“']:
            guest = speaker
            break
    
    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º
    keywords = []
    if 'å††' in early_text:
        money_match = re.search(r'(\d+[ä¸‡åƒå„„]?å††)', early_text)
        if money_match:
            keywords.append(money_match.group(1))
    
    numbers = re.findall(r'(\d+[%ï¼…]|\d+å€|\d+å€‹|\d+äºº)', early_text)
    keywords.extend(numbers[:2])
    
    # è¦ç´„ä½œæˆ
    summary_parts = []
    
    if guest:
        summary_parts.append(f"{guest}ã‚’ã‚²ã‚¹ãƒˆã«è¿ãˆãŸã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã€‚")
    
    # ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰ä¸»è¦ãƒ†ãƒ¼ãƒã‚’æŠ½å‡º
    if 'ã€œ' in title:
        theme = title.split('ã€œ')[0].split('ï¼')[0]
        summary_parts.append(f"{theme}ã«ã¤ã„ã¦æ·±ãæ˜ã‚Šä¸‹ã’ã‚‹ã€‚")
    
    if keywords:
        summary_parts.append(f"è©±é¡Œã®ä¸­å¿ƒã¯{', '.join(keywords[:2])}ãªã©ã€‚")
    
    summary_parts.append("ç§‘å­¦ã¨ãƒ“ã‚¸ãƒã‚¹ã®è¦–ç‚¹ã‹ã‚‰é£Ÿã®ä¸–ç•Œã‚’æ¢æ±‚ã™ã‚‹å†…å®¹ã¨ãªã£ã¦ã„ã‚‹ã€‚")
    
    return '\n\n'.join(summary_parts)


def generate_timeline(entries: list) -> str:
    """ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ã‚’ç”Ÿæˆ"""
    timeline = ["# ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ç›®æ¬¡\n"]
    
    # 5åˆ†ã”ã¨ã«ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ä½œæˆ
    section_interval = 300  # 5åˆ† = 300ç§’
    current_section = 0
    section_texts = []
    
    for entry in entries:
        time_parts = entry['time'].split(':')
        total_seconds = int(time_parts[0]) * 3600 + int(time_parts[1]) * 60 + int(time_parts[2])
        section = total_seconds // section_interval
        
        if section > current_section:
            # ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’ç”Ÿæˆ
            if section_texts:
                # æœ€ã‚‚ç‰¹å¾´çš„ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æ¢ã™
                combined_text = ' '.join(section_texts)
                
                # æ•°å­—ã‚’å«ã‚€è¡¨ç¾ã‚’å„ªå…ˆ
                number_match = re.search(r'(\d+[ä¸‡åƒå„„]?å††|\d+[%ï¼…]|\d+å€|\d+å€‹)', combined_text)
                if number_match:
                    highlight = number_match.group(1)
                    timeline.append(f"{current_section * 5:02d}:00 {highlight}ãŒè©±é¡Œã«")
                else:
                    # ç–‘å•æ–‡ã‚’æ¢ã™
                    question_match = re.search(r'([^ã€‚ï¼Ÿï¼]+[ï¼Ÿ?])', combined_text)
                    if question_match:
                        timeline.append(f"{current_section * 5:02d}:00 {question_match.group(1)}")
                    else:
                        # ä¸€èˆ¬çš„ãªãƒˆãƒ”ãƒƒã‚¯
                        timeline.append(f"{current_section * 5:02d}:00 è©±é¡Œã¯ç¶šã")
            
            current_section = section
            section_texts = []
        
        section_texts.append(entry['text'])
    
    return '\n'.join(timeline)


def generate_keywords(entries: list, metadata: dict) -> str:
    """ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º"""
    full_text = ' '.join([e['text'] for e in entries])
    
    keywords = {
        'äººç‰©': [],
        'å°‚é–€ç”¨èª': [],
        'ä¼æ¥­ãƒ»ã‚µãƒ¼ãƒ“ã‚¹': [],
        'ãƒˆãƒ”ãƒƒã‚¯ãƒ»æ¦‚å¿µ': [],
        'æ•°å­—ãƒ»ãƒ‡ãƒ¼ã‚¿': []
    }
    
    # äººç‰©
    person_pattern = r'([ã-ã‚“ã‚¡-ãƒ¶ãƒ¼ä¸€-é¾ ]+)ã•ã‚“'
    persons = re.findall(person_pattern, full_text)
    keywords['äººç‰©'] = list(set([p for p in persons if len(p) > 1]))[:5]
    
    # æ•°å­—ãƒ»ãƒ‡ãƒ¼ã‚¿
    numbers = re.findall(r'(\d+[ä¸‡åƒå„„]?å††|\d+[%ï¼…]|\d+å€|\d+å€‹|\d+äºº)', full_text)
    keywords['æ•°å­—ãƒ»ãƒ‡ãƒ¼ã‚¿'] = list(set(numbers))[:5]
    
    # ä¼æ¥­åï¼ˆã‚«ã‚¿ã‚«ãƒŠï¼‰
    companies = re.findall(r'([ã‚¡-ãƒ¶ãƒ¼]{3,})', full_text)
    keywords['ä¼æ¥­ãƒ»ã‚µãƒ¼ãƒ“ã‚¹'] = list(set([c for c in companies if len(c) < 10]))[:5]
    
    # å°‚é–€ç”¨èªï¼ˆæ¼¢å­—ãŒå¤šã„ï¼‰
    terms = re.findall(r'([ä¸€-é¾ ]{3,6})', full_text)
    keywords['å°‚é–€ç”¨èª'] = list(set(terms))[:10]
    
    # ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
    result = []
    for category, words in keywords.items():
        if words:
            result.append(f"## {category}")
            for word in words:
                result.append(f"- {word}")
            result.append("")
    
    return '\n'.join(result)


def generate_instagram_post(entries: list, metadata: dict) -> str:
    """InstagramæŠ•ç¨¿æ–‡ã‚’ç”Ÿæˆ"""
    title = metadata.get('title', '')
    
    # æœ€åˆã®èˆˆå‘³æ·±ã„æ•°å­—ã‚„äº‹å®Ÿã‚’æ¢ã™
    highlight = None
    for entry in entries[:30]:
        number_match = re.search(r'(\d+[ä¸‡åƒå„„]?å††|\d+[%ï¼…])', entry['text'])
        if number_match:
            highlight = number_match.group(1)
            break
    
    post = []
    if highlight:
        post.append(f"ã€{highlight}ã€‘ã£ã¦çŸ¥ã£ã¦ã¾ã—ãŸï¼ŸğŸ¤”")
    else:
        post.append("ã€ä»Šé€±ã®KNOWãƒ•ãƒ¼ãƒ‰ãƒ©ã‚¸ã‚ªã€‘ğŸ™ï¸")
    
    post.append("")
    post.append(f"ä»Šå›ã®ãƒ†ãƒ¼ãƒã¯\nã€Œ{title.split('ï¼')[0] if 'ï¼' in title else title}ã€")
    post.append("")
    post.append("è©³ã—ãã¯ãƒãƒƒãƒ‰ã‚­ãƒ£ã‚¹ãƒˆã§ï¼")
    post.append("ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ã®ãƒªãƒ³ã‚¯ã‹ã‚‰è´ã‘ã¾ã™ğŸ“»")
    post.append("")
    post.append("#KNOWãƒ•ãƒ¼ãƒ‰ãƒ©ã‚¸ã‚ª #ãƒãƒƒãƒ‰ã‚­ãƒ£ã‚¹ãƒˆ #é£Ÿã®ç§‘å­¦")
    
    return '\n'.join(post)


def generate_x_post(entries: list, metadata: dict) -> str:
    """XæŠ•ç¨¿æ–‡ã‚’ç”Ÿæˆ"""
    title = metadata.get('title', '')
    
    # æ•°å­—ã‚’å«ã‚€èˆˆå‘³æ·±ã„äº‹å®Ÿã‚’æ¢ã™
    for entry in entries[:20]:
        number_match = re.search(r'(\d+[ä¸‡åƒå„„]?å††|\d+[%ï¼…])', entry['text'])
        if number_match:
            return f"{number_match.group(1)}!? ä»Šé€±ã®KNOWãƒ•ãƒ¼ãƒ‰ãƒ©ã‚¸ã‚ªã¯è¡æ’ƒã®å†…å®¹ã§ã—ãŸã€‚\n\n{title}\n\n#KNOWãƒ•ãƒ¼ãƒ‰ãƒ©ã‚¸ã‚ª"
    
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
    short_title = title.split('ï¼')[0] if 'ï¼' in title else title
    return f"ğŸ™ï¸ {short_title}\n\nä»Šé€±ã®KNOWãƒ•ãƒ¼ãƒ‰ãƒ©ã‚¸ã‚ªé…ä¿¡ä¸­ï¼\n#KNOWãƒ•ãƒ¼ãƒ‰ãƒ©ã‚¸ã‚ª"


def generate_quotes(entries: list) -> str:
    """å°è±¡çš„ãªå¼•ç”¨ã‚’æŠ½å‡º"""
    quotes = ["# å°è±¡çš„ãªå¼•ç”¨é›†\n"]
    
    # ç–‘å•æ–‡ã‚’æ¢ã™
    questions = []
    for entry in entries:
        if 'ï¼Ÿ' in entry['text'] or '?' in entry['text']:
            if 10 < len(entry['text']) < 50:
                questions.append(entry['text'])
    
    if questions:
        quotes.append("## èˆˆå‘³æ·±ã„å•ã„ã‹ã‘")
        for q in questions[:5]:
            quotes.append(f"ã€Œ{q}ã€")
            quotes.append("")
    
    # æ•°å­—ã‚’å«ã‚€ç™ºè¨€
    number_quotes = []
    for entry in entries:
        if re.search(r'\d+[ä¸‡åƒå„„]?å††|\d+[%ï¼…]|\d+å€', entry['text']):
            if 20 < len(entry['text']) < 80:
                number_quotes.append(entry['text'])
    
    if number_quotes:
        quotes.append("## è¡æ’ƒã®æ•°å­—")
        for nq in number_quotes[:3]:
            quotes.append(f"ã€Œ{nq}ã€")
            quotes.append("")
    
    return '\n'.join(quotes)


def generate_links(metadata: dict) -> str:
    """é–¢é€£ãƒªãƒ³ã‚¯é›†ã‚’ç”Ÿæˆ"""
    links = ["# é–¢é€£ãƒªãƒ³ã‚¯é›†\n"]
    
    links.append("## ç•ªçµ„é–¢é€£")
    links.append("- KNOWãƒ•ãƒ¼ãƒ‰ãƒ©ã‚¸ã‚ª Instagram: @knowfoodradio")
    links.append("- ãŠä¾¿ã‚Šãƒ•ã‚©ãƒ¼ãƒ : [URL]")
    links.append("- Spotifyãƒ—ãƒ¬ã‚¤ãƒªã‚¹ãƒˆ: [URL]")
    links.append("")
    
    # ã‚²ã‚¹ãƒˆãŒã„ã‚‹å ´åˆ
    title = metadata.get('title', '')
    if 'ï¼' in title:
        guest_info = title.split('ï¼')[1]
        if 'ã•ã‚“' in guest_info:
            links.append("## ã‚²ã‚¹ãƒˆé–¢é€£")
            links.append(f"- {guest_info}ã®æƒ…å ±: [URL]")
            links.append("")
    
    return '\n'.join(links)


def process_episode(episode_dir: Path) -> bool:
    """ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®æ´¾ç”Ÿãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆ"""
    print(f"å‡¦ç†ä¸­: {episode_dir.name}")
    
    # æ–‡å­—èµ·ã“ã—ãƒ•ã‚¡ã‚¤ãƒ«ã®ç¢ºèª
    transcript_path = episode_dir / "transcript.srt"
    if not transcript_path.exists():
        print("  âš ï¸  æ–‡å­—èµ·ã“ã—ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return False
    
    # æ—¢ã«å‡¦ç†æ¸ˆã¿ã‹ãƒã‚§ãƒƒã‚¯
    if (episode_dir / "summary.md").exists():
        print("  â­ï¸  æ—¢ã«å‡¦ç†æ¸ˆã¿")
        return True
    
    try:
        # ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
        with open(transcript_path, 'r', encoding='utf-8') as f:
            srt_content = f.read()
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        metadata_path = episode_dir / "metadata.json"
        metadata = {}
        if metadata_path.exists():
            with open(metadata_path, 'r', encoding='utf-8') as f:
                metadata = json.load(f)
        
        # SRTãƒ‘ãƒ¼ã‚¹
        entries = parse_srt(srt_content)
        if not entries:
            print("  âš ï¸  æ–‡å­—èµ·ã“ã—ã®è§£æã«å¤±æ•—")
            return False
        
        # å„ç¨®ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆ
        files_to_generate = {
            'summary.md': generate_summary(entries, metadata),
            'timeline.md': generate_timeline(entries),
            'keywords.txt': generate_keywords(entries, metadata),
            'social/instagram.txt': generate_instagram_post(entries, metadata),
            'social/x.txt': generate_x_post(entries, metadata),
            'quotes.txt': generate_quotes(entries),
            'links.md': generate_links(metadata)
        }
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        for file_name, content in files_to_generate.items():
            file_path = episode_dir / file_name
            file_path.parent.mkdir(parents=True, exist_ok=True)
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(content)
            print(f"  âœ“ {file_name}")
        
        return True
        
    except Exception as e:
        print(f"  âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        return False


def main():
    # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã®å‡¦ç†
    import argparse
    parser = argparse.ArgumentParser(description='ç°¡æ˜“ç‰ˆæ´¾ç”Ÿãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ')
    parser.add_argument('--limit', type=int, default=30, help='å‡¦ç†ã™ã‚‹ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°ã®ä¸Šé™')
    args = parser.parse_args()
    
    # æ´¾ç”Ÿãƒ†ã‚­ã‚¹ãƒˆãŒãªã„ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’æ¤œç´¢
    episodes_to_process = []
    for episode_dir in FROM_RSS_DIR.iterdir():
        if episode_dir.is_dir() and not episode_dir.name.startswith('.'):
            if (episode_dir / "transcript.srt").exists() and not (episode_dir / "summary.md").exists():
                episodes_to_process.append(episode_dir)
    
    # å‡¦ç†æ•°ã‚’åˆ¶é™
    total_episodes = len(episodes_to_process)
    episodes_to_process = episodes_to_process[:args.limit]
    
    print(f"å‡¦ç†å¯¾è±¡: {len(episodes_to_process)} ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ï¼ˆå…¨{total_episodes}ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ä¸­ï¼‰")
    
    # å„ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’å‡¦ç†
    success_count = 0
    for i, episode_dir in enumerate(episodes_to_process):
        print(f"\n[{i+1}/{len(episodes_to_process)}]", end=" ")
        if process_episode(episode_dir):
            success_count += 1
    
    print(f"\n\nå®Œäº†: {success_count}/{len(episodes_to_process)} ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’å‡¦ç†ã—ã¾ã—ãŸ")
    if total_episodes > args.limit:
        print(f"æ®‹ã‚Š {total_episodes - args.limit} ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã¯æœªå‡¦ç†ã§ã™")


if __name__ == '__main__':
    main()