#!/usr/bin/env python3
"""
æ–‡å­—èµ·ã“ã—ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆtranscript.srtï¼‰ã‹ã‚‰ã€Œã¸ã‡ã€ã€Œã¸ãƒ¼ã€ã€Œã¸ãˆã€ãªã©ã®
ãƒªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’æŠ½å‡ºã—ã¦ã€ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã”ã¨ã«ã‚«ã‚¦ãƒ³ãƒˆã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import os
import re
from pathlib import Path
from typing import List, Dict, Tuple
import argparse
import json


class HeeAnalyzer:
    """ã€Œã¸ã‡ã€ãƒªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’è§£æã™ã‚‹ã‚¯ãƒ©ã‚¹"""

    def __init__(self, context_blocks: int = 0):
        """
        Args:
            context_blocks: å‰å¾Œã«å«ã‚ã‚‹ãƒ–ãƒ­ãƒƒã‚¯æ•°ï¼ˆ0=å‰å¾Œãªã—ã€2=å‰å¾Œ2ãƒ–ãƒ­ãƒƒã‚¯ãšã¤ï¼‰
        """
        # ã‚¹ãƒšãƒ¼ã‚¹ãƒ»æ”¹è¡Œãƒ»å…¨è§’ã‚¹ãƒšãƒ¼ã‚¹ã‚’å«ã‚€å¯èƒ½æ€§ã‚’è€ƒæ…®ã—ãŸã€Œã¸ã‡ã€ãƒ‘ã‚¿ãƒ¼ãƒ³
        # ä¾‹: "ã¸ ãˆ"ã€"ã¸\nãˆ"ã€"ã¸ã€€ãƒ¼"ã€"ã¸ãˆãˆ" ãªã©
        #
        # ãƒ‘ã‚¿ãƒ¼ãƒ³è¨­è¨ˆã®è€ƒæ…®äº‹é …:
        # 1. 1æ–‡å­—ã”ã¨ã«ã‚¹ãƒšãƒ¼ã‚¹ãŒå…¥ã‚‹: "ã¸ ãˆ"
        # 2. æ”¹è¡ŒãŒå…¥ã‚‹: "ã¸\nãˆ"
        # 3. å…¨è§’ã‚¹ãƒšãƒ¼ã‚¹ãŒå…¥ã‚‹: "ã¸ã€€ãˆ"
        # 4. è¤‡æ•°ã®ã€Œãˆ/ãƒ¼/ã‡ã€ãŒç¶šã: "ã¸ãˆãˆ"ã€"ã¸ãˆãƒ¼"
        # 5. ã‚¹ãƒšãƒ¼ã‚¹ã¨æ”¹è¡ŒãŒæ··åœ¨: "ã¸ \n ãˆ"
        self.hee_patterns = [
            # ã²ã‚‰ãŒãªç‰ˆ: ã¸ + (ã‡/ãˆ/ãƒ¼)ã®çµ„ã¿åˆã‚ã›
            # [\s\u3000\n]* = åŠè§’ã‚¹ãƒšãƒ¼ã‚¹ã€å…¨è§’ã‚¹ãƒšãƒ¼ã‚¹(\u3000)ã€æ”¹è¡Œ(\n)ã®ã„ãšã‚Œã‹ãŒ0å›ä»¥ä¸Š
            # [ã‡ãˆãƒ¼] ãŒ1ã€œ3å›ç¹°ã‚Šè¿”ã•ã‚Œã‚‹ï¼ˆã¸ãˆãˆ ã¾ã§å¯¾å¿œï¼‰
            r'ã¸[\s\u3000\n]*[ã‡ãˆãƒ¼][\s\u3000\n]*[ã‡ãˆãƒ¼]?[\s\u3000\n]*[ã‡ãˆãƒ¼]?',

            # ã‚«ã‚¿ã‚«ãƒŠç‰ˆ: ãƒ˜ + (ã‚§/ã‚¨/ãƒ¼)ã®çµ„ã¿åˆã‚ã›
            r'ãƒ˜[\s\u3000\n]*[ã‚§ã‚¨ãƒ¼][\s\u3000\n]*[ã‚§ã‚¨ãƒ¼]?[\s\u3000\n]*[ã‚§ã‚¨ãƒ¼]?',
        ]
        self.compiled_patterns = [re.compile(p, re.IGNORECASE) for p in self.hee_patterns]
        self.context_blocks = context_blocks

    def extract_text_from_srt(self, srt_path: Path) -> str:
        """SRTãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆéƒ¨åˆ†ã®ã¿ã‚’æŠ½å‡º"""
        with open(srt_path, 'r', encoding='utf-8') as f:
            lines = f.readlines()

        text_parts = []
        skip_next = False

        for line in lines:
            line = line.strip()

            # ç©ºè¡Œã¯ã‚¹ã‚­ãƒƒãƒ—
            if not line:
                skip_next = False
                continue

            # ç•ªå·è¡Œï¼ˆæ•°å­—ã®ã¿ï¼‰ã‚’ã‚¹ã‚­ãƒƒãƒ—
            if line.isdigit():
                skip_next = True
                continue

            # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—è¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—
            if '-->' in line:
                skip_next = True
                continue

            # ãƒ†ã‚­ã‚¹ãƒˆè¡Œã‚’åé›†
            if not skip_next:
                text_parts.append(line)

        return ' '.join(text_parts)

    def find_hee_with_context(self, srt_path: Path, context_blocks: int = 0) -> List[Dict[str, str]]:
        """ã€Œã¸ã‡ã€ãŒå«ã¾ã‚Œã‚‹ç®‡æ‰€ã‚’ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã¨ã¨ã‚‚ã«æŠ½å‡º

        Args:
            srt_path: SRTãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
            context_blocks: å‰å¾Œã«å«ã‚ã‚‹ãƒ–ãƒ­ãƒƒã‚¯æ•°ï¼ˆ0=å‰å¾Œãªã—ã€1=å‰å¾Œ1ãƒ–ãƒ­ãƒƒã‚¯ãšã¤ã€2=å‰å¾Œ2ãƒ–ãƒ­ãƒƒã‚¯ãšã¤ï¼‰
        """
        results = []

        with open(srt_path, 'r', encoding='utf-8') as f:
            content = f.read()

        # SRTã®ãƒ–ãƒ­ãƒƒã‚¯ã”ã¨ã«åˆ†å‰²ï¼ˆç©ºè¡Œã§åŒºåˆ‡ã‚‰ã‚Œã‚‹ï¼‰
        blocks = content.strip().split('\n\n')

        # å„ãƒ–ãƒ­ãƒƒã‚¯ã‚’ãƒ‘ãƒ¼ã‚¹
        parsed_blocks = []
        for block in blocks:
            lines = block.strip().split('\n')
            if len(lines) < 3:
                continue

            # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—è¡Œã‚’æ¢ã™
            timestamp = None
            text_lines = []

            for line in lines:
                if '-->' in line:
                    timestamp = line.strip()
                elif not line.isdigit():  # ç•ªå·è¡Œã§ãªã„
                    text_lines.append(line.strip())

            if timestamp:
                parsed_blocks.append({
                    'timestamp': timestamp,
                    'text': ' '.join(text_lines)
                })

        # ã€Œã¸ã‡ã€ã‚’å«ã‚€ãƒ–ãƒ­ãƒƒã‚¯ã‚’æ¤œç´¢
        for i, block_data in enumerate(parsed_blocks):
            full_text = block_data['text']

            # ã€Œã¸ã‡ã€ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ãƒã‚§ãƒƒã‚¯
            matched = False
            for pattern in self.compiled_patterns:
                if pattern.search(full_text):
                    matched = True
                    break

            if not matched:
                continue

            # å‰å¾Œã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’è¿½åŠ 
            start_idx = max(0, i - context_blocks)
            end_idx = min(len(parsed_blocks), i + context_blocks + 1)

            context_texts = []
            for j in range(start_idx, end_idx):
                context_texts.append(parsed_blocks[j]['text'])

            context_text = ' '.join(context_texts)

            results.append({
                'timestamp': block_data['timestamp'],
                'text': full_text,
                'cleaned_text': full_text.replace(' ', '').replace('ã€€', ''),
                'context': context_text.replace(' ', '').replace('ã€€', '') if context_blocks > 0 else None
            })

        return results

    def analyze_episode(self, episode_dir: Path) -> Dict:
        """1ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’è§£æ"""
        srt_path = episode_dir / 'transcript.srt'

        if not srt_path.exists():
            return None

        hee_instances = self.find_hee_with_context(srt_path, self.context_blocks)

        return {
            'episode_name': episode_dir.name,
            'count': len(hee_instances),
            'instances': hee_instances
        }

    def analyze_all_episodes(self, from_rss_dir: Path, sort_by: str = 'count-desc') -> List[Dict]:
        """
        from-rssé…ä¸‹ã®å…¨ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’è§£æ

        Args:
            from_rss_dir: from-rssãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹
            sort_by: ã‚½ãƒ¼ãƒˆé † ('count-desc', 'count-asc', 'name')
        """
        results = []

        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’åå‰é †ã«ã‚½ãƒ¼ãƒˆ
        episode_dirs = sorted([d for d in from_rss_dir.iterdir() if d.is_dir()])

        for episode_dir in episode_dirs:
            result = self.analyze_episode(episode_dir)
            if result:
                results.append(result)

        # ã‚½ãƒ¼ãƒˆå‡¦ç†
        if sort_by == 'count-desc':
            # ã¸ã‡å›æ•°ã®å¤šã„é †ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
            results.sort(key=lambda x: x['count'], reverse=True)
        elif sort_by == 'count-asc':
            # ã¸ã‡å›æ•°ã®å°‘ãªã„é †
            results.sort(key=lambda x: x['count'])
        elif sort_by == 'name':
            # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰åé †ï¼ˆå…ƒã®é †åºï¼‰
            results.sort(key=lambda x: x['episode_name'])

        return results

    def print_summary(self, results: List[Dict], show_details: bool = False):
        """çµæœã‚µãƒãƒªã‚’å‡ºåŠ›"""
        print("=" * 80)
        print("ã€Œã¸ã‡ã€ãƒªã‚¢ã‚¯ã‚·ãƒ§ãƒ³è§£æçµæœ")
        print("=" * 80)
        print()

        total_count = sum(r['count'] for r in results)
        episodes_with_hee = sum(1 for r in results if r['count'] > 0)

        print(f"è§£æã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°: {len(results)}")
        print(f"ã€Œã¸ã‡ã€ã‚’å«ã‚€ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°: {episodes_with_hee}")
        print(f"ã€Œã¸ã‡ã€ç·å‡ºç¾å›æ•°: {total_count}")
        print()

        # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã”ã¨ã®é›†è¨ˆ
        print("-" * 80)
        print("ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã”ã¨ã®ã€Œã¸ã‡ã€ã‚«ã‚¦ãƒ³ãƒˆ")
        print("-" * 80)

        for result in results:
            if result['count'] > 0:
                print(f"[{result['count']:2d}å›] {result['episode_name']}")

                if show_details and result['instances']:
                    for instance in result['instances']:
                        # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’è¦‹ã‚„ã™ãæ•´å½¢
                        time_range = instance['timestamp'].split(' --> ')
                        start_time = time_range[0] if len(time_range) > 0 else ''

                        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãŒã‚ã‚Œã°ãã‚Œã‚’è¡¨ç¤ºã€ãªã‘ã‚Œã°cleaned_text
                        display_text = instance.get('context') or instance['cleaned_text']
                        print(f"  â”” {start_time}: {display_text[:100]}...")
                    print()

        print()
        print("=" * 80)

    def export_to_json(self, results: List[Dict], output_path: Path):
        """çµæœã‚’JSONå½¢å¼ã§ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"""
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        print(f"çµæœã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«å‡ºåŠ›ã—ã¾ã—ãŸ: {output_path}")

    def export_to_markdown(self, results: List[Dict], output_path: Path):
        """çµæœã‚’Markdownå½¢å¼ã§ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"""
        with open(output_path, 'w', encoding='utf-8') as f:
            # ãƒ˜ãƒƒãƒ€ãƒ¼
            f.write("# ã€Œã¸ã‡ã€ãƒªã‚¢ã‚¯ã‚·ãƒ§ãƒ³è§£æçµæœ\n\n")

            # ã‚µãƒãƒª
            total_count = sum(r['count'] for r in results)
            episodes_with_hee = sum(1 for r in results if r['count'] > 0)

            f.write("## ğŸ“Š ã‚µãƒãƒª\n\n")
            f.write(f"- **è§£æã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°**: {len(results)}\n")
            f.write(f"- **ã€Œã¸ã‡ã€ã‚’å«ã‚€ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°**: {episodes_with_hee}\n")
            f.write(f"- **ã€Œã¸ã‡ã€ç·å‡ºç¾å›æ•°**: {total_count}\n\n")

            # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã”ã¨ã®è©³ç´°
            f.write("## ğŸ“ ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã”ã¨ã®è©³ç´°\n\n")

            for result in results:
                if result['count'] == 0:
                    continue

                f.write(f"### {result['episode_name']}\n\n")
                f.write(f"**å‡ºç¾å›æ•°**: {result['count']}å›\n\n")

                for i, instance in enumerate(result['instances'], 1):
                    time_range = instance['timestamp'].split(' --> ')
                    start_time = time_range[0] if len(time_range) > 0 else ''

                    f.write(f"#### {i}. ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—: `{start_time}`\n\n")

                    # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãŒã‚ã‚Œã°ãã‚Œã‚’è¡¨ç¤ºã€ãªã‘ã‚Œã°cleaned_text
                    if instance.get('context'):
                        f.write(f"> {instance['context']}\n\n")
                    else:
                        f.write(f"> {instance['cleaned_text']}\n\n")

                f.write("---\n\n")

            # ãƒ•ãƒƒã‚¿ãƒ¼
            f.write("## ğŸ“Œ æ³¨æ„\n\n")
            f.write("ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯ `analyze_hee.py` ã«ã‚ˆã£ã¦è‡ªå‹•ç”Ÿæˆã•ã‚Œã¾ã—ãŸã€‚\n")

        print(f"çµæœã‚’Markdownãƒ•ã‚¡ã‚¤ãƒ«ã«å‡ºåŠ›ã—ã¾ã—ãŸ: {output_path}")


def main():
    parser = argparse.ArgumentParser(
        description='æ–‡å­—èµ·ã“ã—ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã€Œã¸ã‡ã€ãƒªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’æŠ½å‡ºãƒ»é›†è¨ˆã™ã‚‹ãƒ„ãƒ¼ãƒ«'
    )
    parser.add_argument(
        'episode',
        nargs='?',
        help='ç‰¹å®šã®ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰åï¼ˆä¾‹: "#32.æ©Ÿèƒ½æ€§è¡¨ç¤ºé£Ÿå“ã¯èª°ã®ãŸã‚ï¼Ÿé–‹ç™ºè€…ç›®ç·šã§è¦‹ã‚‹ãƒˆã‚¯ãƒ›ã¨ã®é•ã„"ï¼‰'
    )
    parser.add_argument(
        '--all',
        action='store_true',
        help='å…¨ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’è§£æ'
    )
    parser.add_argument(
        '--details',
        action='store_true',
        help='è©³ç´°æƒ…å ±ï¼ˆã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã¨æ–‡è„ˆï¼‰ã‚’è¡¨ç¤º'
    )
    parser.add_argument(
        '--json',
        type=str,
        help='çµæœã‚’JSONå½¢å¼ã§å‡ºåŠ›ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’æŒ‡å®šï¼‰'
    )
    parser.add_argument(
        '--markdown',
        type=str,
        help='çµæœã‚’Markdownå½¢å¼ã§å‡ºåŠ›ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’æŒ‡å®šï¼‰'
    )
    parser.add_argument(
        '--context',
        type=int,
        default=2,
        help='å‰å¾Œã«å«ã‚ã‚‹ãƒ–ãƒ­ãƒƒã‚¯æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 2ï¼‰ã€‚0ã«ã™ã‚‹ã¨å‰å¾Œãªã—'
    )
    parser.add_argument(
        '--sort',
        type=str,
        default='count-desc',
        choices=['count-desc', 'count-asc', 'name'],
        help='ä¸¦ã³é †ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: count-descï¼‰ã€‚count-desc=ã¸ã‡å›æ•°ã®å¤šã„é †ã€count-asc=ã¸ã‡å›æ•°ã®å°‘ãªã„é †ã€name=ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰åé †'
    )

    args = parser.parse_args()

    # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã®è¨­å®š
    script_dir = Path(__file__).parent
    from_rss_dir = script_dir / 'from-rss'

    if not from_rss_dir.exists():
        print(f"ã‚¨ãƒ©ãƒ¼: from-rssãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {from_rss_dir}")
        return

    analyzer = HeeAnalyzer(context_blocks=args.context)

    # å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰ã®åˆ¤å®š
    if args.all:
        # å…¨ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰è§£æ
        print("å…¨ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’è§£æä¸­...")
        results = analyzer.analyze_all_episodes(from_rss_dir, sort_by=args.sort)
    elif args.episode:
        # ç‰¹å®šã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰è§£æ
        episode_dir = from_rss_dir / args.episode
        if not episode_dir.exists():
            print(f"ã‚¨ãƒ©ãƒ¼: ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {episode_dir}")
            # å€™è£œã‚’è¡¨ç¤º
            print("\nåˆ©ç”¨å¯èƒ½ãªã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰:")
            for d in sorted(from_rss_dir.iterdir()):
                if d.is_dir():
                    print(f"  - {d.name}")
            return

        result = analyzer.analyze_episode(episode_dir)
        results = [result] if result else []
    else:
        # å¼•æ•°ãªã—ã®å ´åˆã¯ãƒ˜ãƒ«ãƒ—ã‚’è¡¨ç¤º
        parser.print_help()
        print("\nä½¿ç”¨ä¾‹:")
        print("  # å…¨ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’è§£æ")
        print("  python analyze_hee.py --all")
        print()
        print("  # è©³ç´°æƒ…å ±ã‚‚è¡¨ç¤º")
        print("  python analyze_hee.py --all --details")
        print()
        print("  # ç‰¹å®šã®ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’è§£æ")
        print('  python analyze_hee.py "#32.æ©Ÿèƒ½æ€§è¡¨ç¤ºé£Ÿå“ã¯èª°ã®ãŸã‚ï¼Ÿé–‹ç™ºè€…ç›®ç·šã§è¦‹ã‚‹ãƒˆã‚¯ãƒ›ã¨ã®é•ã„"')
        print()
        print("  # JSONå½¢å¼ã§å‡ºåŠ›")
        print("  python analyze_hee.py --all --json hee_analysis.json")
        return

    # çµæœã®è¡¨ç¤º
    analyzer.print_summary(results, show_details=args.details)

    # JSONå‡ºåŠ›
    if args.json:
        output_path = Path(args.json)
        analyzer.export_to_json(results, output_path)

        # JSONå‡ºåŠ›æ™‚ã«è‡ªå‹•çš„ã«Markdownã‚‚ç”Ÿæˆï¼ˆåŒã˜ãƒ•ã‚¡ã‚¤ãƒ«åã§æ‹¡å¼µå­ã‚’.mdã«å¤‰æ›´ï¼‰
        md_path = output_path.with_suffix('.md')
        analyzer.export_to_markdown(results, md_path)

    # Markdownå‡ºåŠ›ï¼ˆå˜ç‹¬æŒ‡å®šã®å ´åˆï¼‰
    if args.markdown:
        output_path = Path(args.markdown)
        analyzer.export_to_markdown(results, output_path)


if __name__ == '__main__':
    main()
